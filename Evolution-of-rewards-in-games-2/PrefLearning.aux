\relax 
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Model}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.1}{Game, payoffs, and utilities}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.2}{Learning}}{1}}
\newlabel{Lrule1}{{1}{2}}
\newlabel{Lrule2}{{2}{2}}
\newlabel{logitchoice}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{Fecundity}}{2}}
\newlabel{fec}{{4}{2}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3}{Preference evolution and the set of possible utility functions}}{3}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4}{Analysis of simple cases}}{3}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.1}{Symmetric two-player games}}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{4.1.1}{Locally stable utility functions}}{3}}
\newlabel{cES}{{5}{3}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.2}{Subset of strategies in the Prisoner's Dilemma}}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Classification of behavioral outcomes in the discrete action model amongst the 4 strategies considered.}}{4}}
\newlabel{behOut}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.3}{Analysis of the behavioral interactions between the strategies}}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Generic total payoff in the discrete action model amongst the 4 strategies considered.}}{5}}
\newlabel{payOut}{{2}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Total payoff in the 3 different types of games in the discrete action model amongst the 4 strategies considered.}}{5}}
\newlabel{payOut}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The 7 strategies considered in the discrete action model. A strategy is defined by the outcomes to which it associates a positive or negative utility in the corresponding outcome matrix (top). Cooperation is denoted by $C$ and defection by $D$. In the outcome matrix, the first letter refers to the action of the focal player (row) and the second letter to the action of its opponent (column). For example, the strategy Realistic associates a positive utility to the outcomes that yield positive material payoffs, and negative utility to outcomes yielding negative material payoffs. The strategy Pareto associates a positive utility to the outcome where both players cooperate ($C,C$) and has a negative utility for all other outcomes.}}{6}}
\newlabel{stratDiscrete}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Vector field (gray arrows) and stochastic trajectory (blue line) for the interaction between ``Pareto'' and ``Selfish''. On the $x$-axis is represented the probability that Pareto cooperates ($p_1$), while on the $y$-axis, this is the probability that Selfish cooperates ($p_2$). The stochastic trajectory is started from the center of the state space $(p_1,p_2) = (\frac  {1}{2},\frac  {1}{2})$ and dots on it represent interaction rounds between the players. Circles represent equilibria: a white-filled circle is a source (both associated eigenvalues are positive); a gray-filled circle is a saddle (one positive and one negative associated eigenvalue); a black circle is a sink (both associated eigenvalues are negative).}}{7}}
\newlabel{cS}{{2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Same as Fig.\nonbreakingspace 2\hbox {} but for the interaction between Pareto and Realistic. Here the deterministic mean field equation admits two locally stable equilibria. The two stochastic trajectories of different color correspond to simulation runs that respectively converge to one of the locally stable equilibria.}}{7}}
\newlabel{dspr}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Same as Fig.\nonbreakingspace 2\hbox {} but for the interaction between Realistic and Realistic. The red-filled dot denotes an equilibrium with 0 eigenvalues.}}{8}}
\newlabel{dspr}{{4}{8}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5}{Continuous action space}}{9}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.1}{Normally distributed actions}}{9}}
\newlabel{upCa}{{6}{9}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.2}{Stochastic approximation}}{10}}
\newlabel{RfExp}{{7}{10}}
\newlabel{RfExp2}{{8}{10}}
\newlabel{RfExp3}{{9}{10}}
\newlabel{RfExpH}{{10}{10}}
\newlabel{RfExpH2}{{11}{10}}
\newlabel{RfExp}{{12}{10}}
\newlabel{meanF}{{13}{10}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.3}{Simulations}}{11}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.4}{Bounded action space}}{11}}
\newlabel{logitTr}{{14}{11}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.5}{Baseline simple individual decision problem}}{11}}
\newlabel{cIndSim}{{15}{11}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{19.16248pt}
\newlabel{tocindent2}{27.37495pt}
\newlabel{tocindent3}{0pt}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Continuous learning in a deterministic 1-player game. (A) The function $u_i(a_i)$ to be maximized by the player (eq.\nonbreakingspace 15\hbox {}), with $\beta =0.6$. (B) Typical dynamics of the mean action, $\mu _{i,t+1}$ when the utility function is that in (A).}}{12}}
\newlabel{simIndDyn}{{5}{12}}
